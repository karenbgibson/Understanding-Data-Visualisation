---
title: "Good Reads exploration"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

## Loading and exploring dataset

```{r}
library(tidyverse)
library(janitor)
library(lubridate)
library(ggplot2)
library(e1071)
library(modelr)
library(patchwork)
```

```{r}
books <- read.csv("books.csv") %>% 
  clean_names()
```

```{r}
(books %>% 
  dim())

books_cleaned <- books %>% 
 drop_na(ratings_count) %>% 
  filter(ratings_count != 0) %>% 
  filter(!average_rating %in% c(" Jr./Sam B. Warner",
                             " one of the founding members of this Tolkien website)/Verlyn Flieger/Turgon (=David E. Smith)",
                             " Rawles",
                             " Son & Ferguson"
                            ))
```
The dataset has 11,131 rows and 12 variables. We are not concerned with books that have no ratings or books that have a rating count of 0. These have been removed.

A number of entries in the "average rating" category are strings of text. These are also removed, we now have 11,043 rows in the dataset.

The 12 variables in the dataset are:

```{r}
(books_cleaned %>% 
  names())
```
book_id, isbn and isbn13 are all unique identifiers for each book. We will remove book_id and isbn and use isbn13 as the most appropriate unique identifier.

num_pages has several entries with '0'. This questions the reliability of this variable and so it is removed from our dataset.

```{r}
books_cleaned <- books_cleaned %>% 
  select(-c(book_id, isbn, num_pages))
```


```{r}
(books_cleaned %>% 
  glimpse())
```
This leaves us with 9 variables.

We now tidy these variables to ensure they are of the correct data type for further analysis. 

```{r warning=FALSE}
books_cleaned <- books_cleaned %>% 
  mutate(average_rating = as.double(average_rating),
         language_code = as.factor(language_code),
         publication_date = mdy(publication_date)) %>% 
  drop_na(publication_date)
```

Two entries have incorrect publications dates (31st June and 31st November) given the unreliability of the data, these entries have also been removed.


## Exploring the data: oldest and newest books

```{r}
ten_oldest_books <- books_cleaned %>% 
  arrange(publication_date) %>% 
  head(10)

(plot_ten_oldest <- ten_oldest_books %>% 
  ggplot(aes(x = publication_date, y = average_rating)) +
  geom_col())
```
The ten oldest books in the dataset were published between 1900 and 1928. 
Number of ratings range from 21-332 and average ratings range from 3.91 - 4.35.

```{r}
ten_newest_books <- books_cleaned %>% 
  arrange(desc(publication_date)) %>% 
  head(10)

(plot_ten_newest <- ten_newest_books %>% 
  ggplot(aes(x = publication_date, y = average_rating)) +
  geom_col())
```
The 10 newest books in the dataset were published between July 2018 and March 2020. Rating count ranges from 9-56171 and average ratings range from 3.43 to 4.50.

So far the data is showing that there is a narrow range in average rating, regardless of when the book was published or the ratings count. 

## Exploring the dataset: average ratings ranges

If we round the average rating and compare the number of each rounded rating we get:

```{r}
books_cleaned_average <- books_cleaned %>% 
  mutate(rounded_rating = round(average_rating), .after = average_rating)
 
 
(average_rating_summary <- books_cleaned_average %>% 
    group_by(rounded_rating) %>% 
    summarise(rounded_rating_count = n()) %>% 
    mutate(percentage = round(rounded_rating_count / 
             sum(rounded_rating_count) * 100, 2))
)
```
Almost 92% of books have an average rating of 4. So what does it take to get a below average rating (1-3) or an above average rating (5)?

## Exploring the dataset: below average books

```{r}
(rating_two_or_one <- books_cleaned_average %>% 
  filter(rounded_rating <= 2))
```
With the exception of "Citizen Girl" all of the books with a rating of 2 or 1 have a ratings count of <5 . Citizen Girl has a ratings count of 5415.

This suggests that there is a potential pattern in the number of reviews and the overall average rating. This is explored further by looking a the rating count ranges for each of the average score categories.


```{r}

(rating_count_ranges <- books_cleaned_average %>% 
  group_by(rounded_rating) %>% 
  summarise(min_review_count = min(ratings_count),
            max_review_count = max(ratings_count))
)


```
```{r}
(ratings_boxplot <- books_cleaned_average %>%
  mutate(rounded_rating = as.factor(rounded_rating)) %>% 
  ggplot(aes(x = rounded_rating,
             y= ratings_count)) +
  geom_boxplot())
```

Due to the range of results it is impossible to infer anything specific from these boxplots; however, there appears to be a few outliers that are perhaps skewing our data. This leads to the question, is there a pattern to what makes more people read and review a book?

We can break our data down to look at the top 5% most reviewed books.

```{r}
top_5_percent_ratings_count <- books_cleaned_average %>% 
  mutate(top_5_percent_ratings_count = 
           percent_rank(ratings_count) > 0.95) %>% 
  filter(top_5_percent_ratings_count == TRUE)
```

Summary of the top 5% most reviewed books (552 books):

```{r}
(top_5_percent_ratings_count %>% 
  summary())
```
Perhaps one area to explore is whether there are publishers that get better reviews than others. 

```{r}
(top_5_percent_ratings_count %>% 
  distinct(publisher) %>% 
  arrange(publisher))
```
It looks like there is inconsistent naming of publishers, or often one publishing house has multiple divisions that they publsih under. This means looking at publishing info will require a lot of tidying and is likely not worth it.

## Exploring the data: publishing languages

Let's looks at books that have been published in multiple languages

```{r}
published_more_than_once <- books_cleaned_average %>%
  filter(duplicated(title) | duplicated(title, fromLast = TRUE)) %>%
  arrange(title)

(published_more_than_once %>% 
  distinct(title))
```

How many languages are represented in the books published more than once?

```{r}
(published_more_than_once %>% 
   distinct(language_code))
```
It looks like 9 langauges, but 4 of these are English. We will group "eng", "en-CA", "en-GB" and "en-US" together, to be left with 6 distinct languages:
- Aleut
- English
- French
- German
- Greek
- Spanish

```{r}
#grouping all versions of English together and calculating mean average rating per book, per language.

published_more_than_once <- published_more_than_once %>% 
  mutate(language_code = case_when(
    language_code %in% c("en-CA", "en-GB", "en-US") ~ "eng",
    .default = language_code)
    ) %>% 
    group_by(title, language_code) %>% 
    summarise(average_rating = mean(average_rating))

```
Use our grouped data we can look at books that have been published in multiple languages:

```{r}
published_multiple_languages <- published_more_than_once %>%
  filter(duplicated(title) | duplicated(title, fromLast = TRUE)) %>%
  arrange(title)
  
```

We find that there are 19 books published in multiple languages:

```{r}
(published_multiple_languages %>% 
  distinct(title))
```

Do books do better in different languages, or do they score roughly the same?

```{r}

(mult_languages_pivot <- published_multiple_languages %>%
  pivot_wider(names_from = language_code, values_from = average_rating))

```

```{r}
#adding colour palette

cbb_palette_language <- c("#56B4E9", "#0072B2","#D55E00", "#E69F00", "darkgreen", "#009E73")
```


```{r}

published_multiple_languages %>% 
  ggplot(aes(x = title, y = average_rating,
             fill = language_code)) + 
  geom_col(position = "dodge") +
  scale_fill_manual(values = cbb_palette_language) +
  theme_bw() +
    labs(title = "Books published in multiple languages",
       subtitle = "Comparing ratings across languages") +
       theme(
     text = element_text(size = 10),
     axis.text.x = element_text(size = 6, 
                                angle = 45,
                                hjust = 1),
     axis.text.y = element_text(size = 6))

```
Jane Eyre averaged slightly lower in English than in German (4.11 to 4.12).

Trainspotting scored slightly lower in English than in French (4.03 to 4.09).

Let's add the variations on English back in to see if this changes anything:

```{r}
published_more_than_once_english <- books_cleaned_average %>%
  filter(duplicated(title) | duplicated(title, fromLast = TRUE)) %>%
  arrange(title) %>% 
  group_by(title, language_code) %>% 
  summarise(average_rating = mean(average_rating)) %>% 
  filter(duplicated(title) | duplicated(title, fromLast = TRUE))
  
```
```{r}

(mult_languages_pivot_eng <- published_more_than_once_english %>%
  pivot_wider(names_from = language_code, values_from = average_rating))
```

This hasn't been looked at in any details yet. It appears that there are minor variations, but nothing that stands out. It might be worth exploring this further. This data could perhaps let publishers and authors get an idea of what languages might be worth publishing in. Or if readers of of variation of English tend to give better reviews. 

# Changing tact

Looking at at the average rating per language for all books

```{r}
(language_avg <- books_cleaned_average %>% 
  group_by(language_code) %>%
  summarise(count = n(), mean_avg = mean(average_rating))
)
```
Need to removed langauges with fewest review numbers. Suggest >20.

```{r}

#looking at the average review score for languages that appear >20
(language_avg_20 <- books_cleaned_average %>% 
  group_by(language_code) %>%
  filter(n() > 20) %>% 
  summarise(count = n(), mean_avg = mean(average_rating))
)
```

```{r}
language_avg_20 %>% 
  ggplot(aes(x = language_code, y = mean_avg, fill = language_code)) + 
  geom_col() +
  theme_bw() +
       theme(
     text = element_text(size = 10),
     axis.text.x = element_text(size = 6, 
                                angle = 45,
                                hjust = 1),
     axis.text.y = element_text(size = 6))
```
What is the average across all books?

```{r}
(avg_all_books <- books_cleaned_average %>% 
  summarise(mean_avg = mean(average_rating)))
```
```{r}
(language_avg_20_above_below <- language_avg_20 %>% 
   mutate(mean = 3.943) %>% 
   mutate(above_or_below = if_else(mean_avg > mean, "above", "below"))
)
```

```{r}
above_below_palette <- c("darkgreen", "darkred")
```


```{r}
language_avg_20_above_below %>% 
  filter(language_code != c("eng", "en-US")) %>% 
  ggplot(aes(x = language_code, y = mean_avg, fill = above_or_below)) + 
  geom_col() +
  scale_fill_manual(values = above_below_palette) +
  geom_label(aes(label = round((mean_avg), 2))) +
  theme_bw()
```

Maybe there is something here? Take out en-US and eng to leave us with a more comfortable range of review numbers?

# Number of pages vs ratings count

```{r}

#reproducing cleaned data, this time keeping num_pages

books_cleaned_v2 <- books %>% 
  drop_na(ratings_count) %>% 
  filter(ratings_count != 0) %>% 
  filter(!average_rating %in% c(" Jr./Sam B. Warner",
                             " one of the founding members of this Tolkien website)/Verlyn Flieger/Turgon (=David E. Smith)",
                             " Rawles",
                             " Son & Ferguson"
                            )) %>% 
  select(-c(book_id, isbn)) %>% 
  mutate(num_pages = as.integer(num_pages),
         ratings_count = as.integer(ratings_count)) %>% 
  mutate(average_rating = as.double(average_rating),
         language_code = as.factor(language_code),
         publication_date = mdy(publication_date)) %>% 
  drop_na(publication_date) %>% 
  mutate(publication_year = year(publication_date)) %>% 
  arrange(num_pages)

```

```{r}
books_cleaned_v2 %>% 
ggplot(aes(x = num_pages, y = ratings_count)) +
  geom_point() +
  labs(x = "Number of Pages", y = "Number of Ratings")
```

```{r}
# how many books have <100 pages?

books_cleaned_v2 %>% 
  filter(num_pages <100)

books_cleaned_v2 %>% 
  filter(num_pages == 0)

books_cleaned_v2 %>% 
  filter(num_pages <10,
         num_pages > 0)
  
```

1,010 books have <100 pages, 75 of these have a page count of 0 and 116 have a page count of between 1 & 9. Given the unlikelihood of this many books being this short, it is clear that there are errors in this variable. 

There is no known appropriate number for fewest pages a book may have. To try to eliminate any incorrect data, we remove the lowest 5% of num_pages. 

Our histogram above also show that there are some outlier books that have a high number of pages. There appears to be a steady range of books with 

```{r}

# checking books with >1000 pages

books_cleaned_v2 %>% 
  filter(num_pages >1000)

```
There are 215 books with >1000 pages. Some of these appear to be collections of books, e.g. "The Border Trilogy". For data consistency, we will also remove the lowest 5% of num_pages.

```{r}

# Removing the bottom and top 5% of page count
num_rows <- nrow(books_cleaned_v2)
bottom_percentage <- 0.05
top_percentage <- 0.95

bottom_rows_to_remove <- round(num_rows * bottom_percentage)
top_rows_to_remove <- round(num_rows * (1 - top_percentage))

# Remove the bottom and top rows
trimmed_books_v1 <- books_cleaned_v2[(bottom_rows_to_remove + 1):(num_rows - top_rows_to_remove), , drop = FALSE]
```

```{r}
trimmed_books_v1 %>% 
  ggplot(aes(x = num_pages, y = ratings_count)) +
  geom_point() +
  labs(x = "Number of Pages", y = "Number of Ratings")
```

```{r}
trimmed_books_v2 <- trimmed_books_v1 %>% 
  arrange(ratings_count)
```


```{r}
# Removing the bottom and top 5% of ratings count
num_rows <- nrow(trimmed_books_v2)
bottom <- 0.05
top <- 0.95

bottom_remove <- round(num_rows * bottom)
top_remove <- round(num_rows * (1 - top))

# Remove the bottom and top rows
trimmed_books_v2 <- trimmed_books_v2[(bottom_remove + 1):(num_rows - top_remove), , drop = FALSE]
```

```{r}
trimmed_books_v2 %>% 
  ggplot(aes(x = num_pages, y = ratings_count)) +
  geom_point() +
  labs(x = "Number of Pages", y = "Number of Ratings")
```

```{r}
model <- lm(ratings_count ~ num_pages, data = trimmed_books_v2)
summary(model)
```
```{r}
plot(model)
```

```{r}
data1 <- trimmed_books_v2 %>%
  add_predictions(model) 

data1 %>%
  ggplot(aes(x = num_pages)) +
  geom_point(aes(y = ratings_count)) +
  geom_line(aes(y = pred), col = "red")
```

```{r}
trimmed_books_v2 %>% 
summarise(skewness = skewness(ratings_count, type = 1))
```
The data is highly right skewed. In this case we should either look at the median as our stat, or look at standardising the data.

```{r}
summary(trimmed_books_v2)
```
num_pages now ranges from 51-750.

Use this to add book range categories:

```{r}
trimmed_books_v2_with_range <- trimmed_books_v2 %>% 
  mutate(page_range = case_when(
      num_pages <= 150 ~ "51-150",
      num_pages <= 250 ~ "151-250",
      num_pages <= 350 ~ "251-350",
      num_pages <= 450 ~ "351-450",
      num_pages <= 550 ~ "451-550",
      num_pages <= 650 ~ "551-650",
      num_pages <= 750 ~ "651-750"
  )) %>% 
  mutate(page_range = factor(page_range, 
                             levels = c("51-150",
                                        "151-250", 
                                        "251-350", 
                                        "351-450", 
                                        "451-550", 
                                        "551-650", 
                                        "651-750")))
```

```{r}
trimmed_books_v2_with_range %>% 
  ggplot(aes(x = page_range, y = ratings_count)) +
  geom_col()
```
```{r}
# writing clean data

trimmed_books_v2_with_range %>%  write.csv("books_cleaned.csv")
```

```{r}
(trimmed_v2_range_summary <- trimmed_books_v2_with_range %>% 
  group_by(page_range) %>% 
  summarise(median_ratings_count = median(ratings_count),
            num_books = n()))
```
```{r}
recent_publications <- c(2010, 2011, 2012, 2013, 2014)
```

```{r}
top_5_years_v2 <- trimmed_books_v2_with_range %>% 
  group_by(publication_year) %>% 
  summarise(num_books = n()) %>% 
  arrange(desc(num_books)) %>% 
  head(5)
```


```{r}
(trimmed_v2_range_summary_two <- trimmed_books_v2_with_range %>% 
  filter(publication_year %in% top_5_years_v2$publication_year) %>% 
  group_by(page_range, publication_year) %>% 
  summarise(median_ratings_count = median(ratings_count),
            num_books = n()))
```

```{r}
trimmed_v2_range_summary_two %>% 
  ggplot(aes(x = publication_year, y = median_ratings_count,
             colour = page_range)) +
  geom_line()
```
## Percentage of representation in dataset for each page range grouping

```{r}
total_books_v2 <- nrow(trimmed_books_v2_with_range)

(v2_page_range_overall <- trimmed_books_v2_with_range %>% 
  group_by(page_range) %>% 
  summarise(num_books = n()) %>% 
  mutate(percentage = (num_books / total_books_v2) * 100) %>%
  ggplot(aes(x = page_range, y = percentage, 
             fill = page_range)) +
  geom_col() +
  geom_text(aes(label = paste0(round(percentage, 2), "%")),
            vjust = 2) +
  labs(title = "Page count of all books") +
  theme(axis.text.x = element_text(size = 12, angle = 45, hjust = 1)) +
  guides(fill = FALSE))
```

## Top 500 books based on review count

```{r}
(v2_page_range_top_500_review_count <- trimmed_books_v2_with_range %>% 
  arrange(desc(ratings_count)) %>% 
  head(500) %>% 
  group_by(page_range) %>% 
  summarise(num_books = n()) %>%
  mutate(percentage = (num_books / 500) * 100) %>% 
  ungroup() %>% 
  ggplot(aes(x = page_range, y = percentage, fill = page_range)) +
  geom_col() +
  geom_text(aes(label = paste0(round(percentage, 2), "%")), 
            vjust = 2) +
  labs(title = "How often does each page range appear in the 500 most reviewed books?") +
  theme(axis.text.x = element_text(size = 12, angle = 45, hjust = 1)) +
  guides(fill = FALSE))

```
```{r}
#pasting both graphs together for side by side view
v2_page_range_overall + v2_page_range_top_500_review_count
```
## Top 500 books based on average rating

```{r}
(v2_page_range_top_500_avg_rating <- trimmed_books_v2_with_range %>% 
  arrange(desc(average_rating)) %>% 
  head(500) %>% 
  group_by(page_range) %>% 
  summarise(num_books = n()) %>%
  mutate(percentage = (num_books / 500) * 100) %>% 
  ungroup() %>% 
  ggplot(aes(x = page_range, y = percentage, fill = page_range)) +
  geom_col() +
  geom_text(aes(label = paste0(round(percentage, 2), "%")), 
            vjust = 2) +
  labs(title = "How often does each page range appear in the 500 highest reviewed books?") +
  theme(axis.text.x = element_text(size = 12, angle = 45, hjust = 1)) +
  guides(fill = FALSE))

```
```{r}
#pasting both graphs together for side by side view
v2_page_range_overall + v2_page_range_top_500_avg_rating
```

## Recreating above graphs with untrimmed dataset
We are no longer looking for a statistion (e.g. mean or median) so removing outliers may not be necessary:

```{r}
summary(books_cleaned_v2)
```
The num_pages range from 0 to 6576. We remove the top and bottom 5% for the same reasons as before: to remove audio books and anthologies. We can use the previously created trimmed_books_v1 and add the categorical page range variable. 

```{r}
trimmed_books_v1
```
```{r}
trimmed_books_v1_with_range <- trimmed_books_v1 %>% 
  mutate(page_range = case_when(
      num_pages <= 150 ~ "51-150",
      num_pages <= 250 ~ "151-250",
      num_pages <= 350 ~ "251-350",
      num_pages <= 450 ~ "351-450",
      num_pages <= 550 ~ "451-550",
      num_pages <= 650 ~ "551-650",
      num_pages <= 750 ~ "651-750"
  )) %>% 
  mutate(page_range = factor(page_range, 
                             levels = c("51-150",
                                        "151-250", 
                                        "251-350", 
                                        "351-450", 
                                        "451-550", 
                                        "551-650", 
                                        "651-750")))
```

## Percentage of representation in dataset for each page range grouping (where ratings count has not been trimmed)

```{r}
total_books_v1 <- nrow(trimmed_books_v1_with_range)

(v1_page_range_overall <- trimmed_books_v1_with_range %>% 
  group_by(page_range) %>% 
  summarise(num_books = n()) %>% 
  mutate(percentage = (num_books / total_books_v1) * 100) %>%
  ggplot(aes(x = page_range, y = percentage, 
             fill = page_range)) +
  geom_col() +
  geom_text(aes(label = paste0(round(percentage, 2), "%")),
            vjust = 2) +
  labs(title = "Page count of all books") +
  theme(axis.text.x = element_text(size = 12, angle = 45, hjust = 1)) +
  guides(fill = FALSE))
```

## Top 500 books based on review count

```{r}
(v1_page_range_top_500_review_count <- trimmed_books_v1_with_range %>% 
  arrange(desc(ratings_count)) %>% 
  head(500) %>% 
  group_by(page_range) %>% 
  summarise(num_books = n()) %>%
  mutate(percentage = (num_books / 500) * 100) %>% 
  ungroup() %>% 
  ggplot(aes(x = page_range, y = percentage, fill = page_range)) +
  geom_col() +
  geom_text(aes(label = paste0(round(percentage, 2), "%")), 
            vjust = 2) +
  labs(title = "How often does each page range appear in the 500 most reviewed books?") +
  theme(axis.text.x = element_text(size = 12, angle = 45, hjust = 1)) +
  guides(fill = FALSE))

```

```{r}
#pasting both graphs together for side by side view
v1_page_range_overall + v1_page_range_top_500_review_count
```

## Top 500 untrimmed books based on average rating

```{r}
(v1_page_range_top_500_avg_rating <- trimmed_books_v1_with_range %>% 
  arrange(desc(average_rating)) %>% 
  head(500) %>% 
  group_by(page_range) %>% 
  summarise(num_books = n()) %>%
  mutate(percentage = (num_books / 500) * 100) %>% 
  ungroup() %>% 
  ggplot(aes(x = page_range, y = percentage, fill = page_range)) +
  geom_col() +
  geom_text(aes(label = paste0(round(percentage, 2), "%")), 
            vjust = 2) +
  labs(title = "How often does each page range appear in the 500 highest reviewed books?") +
  theme(axis.text.x = element_text(size = 12, angle = 45, hjust = 1)) +
  guides(fill = FALSE))

```

```{r}
#pasting both graphs together for side by side view
v1_page_range_overall + v1_page_range_top_500_avg_rating
```
